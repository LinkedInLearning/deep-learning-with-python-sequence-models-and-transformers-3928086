{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55764ae-20f6-403d-b2d1-fd8b80c78c65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Question Answering with Pretrained Models in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e620a2-9784-4a2b-98e9-219fd0130997",
   "metadata": {},
   "source": [
    "**Question answering (QA)** is a critical application of natural language processing (NLP) that enables machines to provide direct answers to questions based on a given text passage. With deep learning and pretrained models, we can build powerful QA systems that either extract the answer from the text (**extractive QA**) or generate a novel response (**abstractive QA**). In this tutorial, we'll limit our focus to extractive question answering using a Hugging Face pre-trained model.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will be able to:\n",
    "+ **Implement a question asnwering pipeline:** Build and run an extractive question answering pipeline that retrieves answer spans directly from the provided context.\n",
    "+ **Analyze the QA output:** Understand the output, including score metrics and answer spans.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, please ensure that you have:\n",
    "+ A working knowledge of Python, including variables, functions, loops, and basic object-oriented programming.\n",
    "+ Familiarity with deep learning model development in Python using Keras and TensorFlow.\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, `ipywidgets`, and `transformers` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ac08b-2223-46e3-bc42-34eae28fe477",
   "metadata": {},
   "source": [
    "Let's also reduce the log verbosity of the `transformers` package. This ensures that we only get error alerts but not informational logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d34c23f-3d65-4787-9dca-16fd31ff3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8f3e6-9136-47e9-9831-df5fee5d8941",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776aadcd-8a89-4d50-a1f4-b8e3b33c6e49",
   "metadata": {},
   "source": [
    "## 1. Instantiate a Pipeline for Question Answering\n",
    "The first thing we do is import the `pipeline` function from the Hugging Face `transformers` package. Then we instantiate a pipeline object called `answerer` while specifying `\"question-answering\"` as the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b60c79-e57a-4e3a-b4f3-1291378b9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "answerer = pipeline(task = \"question-answering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e925de6-a885-472e-a8e2-1f97f3d781b2",
   "metadata": {},
   "source": [
    "## 2. Answer Questions Based on Provided Text\n",
    "Next, we provide a source document from which questions should be answered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04aad9c-67fa-474e-8062-03831da40dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\"\n",
    "The Apollo missions were a series of space missions conducted by NASA (National Aeronautics \n",
    "and Space Administration) between 1961 and 1972 with the primary objective of landing humans \n",
    "on the Moon and safely returning them to Earth. These missions marked a significant milestone in \n",
    "space exploration, pushing the boundaries of human capability beyond Earth’s atmosphere.\n",
    "\n",
    "The most famous mission, Apollo 11, successfully landed astronauts Neil Armstrong and Buzz Aldrin \n",
    "on the Moon on July 20, 1969, while Michael Collins piloted the command module in lunar orbit. This \n",
    "historic event marked the first time humans set foot on another celestial body, with Armstrong’s \n",
    "famous words: \"That's one small step for man, one giant leap for mankind.\"\n",
    "\n",
    "Beyond the technological and scientific advancements, the Apollo missions provided valuable data \n",
    "about the Moon’s surface, geological composition, and atmosphere. The missions also tested life \n",
    "support systems, spacecraft engineering, and astronaut endurance in deep space. These insights \n",
    "have been critical in shaping future space exploration, including potential human missions to Mars \n",
    "and beyond.\n",
    "\n",
    "The success of the Apollo program demonstrated the feasibility of human space travel and laid the \n",
    "groundwork for subsequent missions like the Space Shuttle program, the International Space \n",
    "Station (ISS), and modern lunar exploration projects like Artemis. Moreover, the knowledge gained \n",
    "from these missions continues to influence discussions on the potential colonization of other \n",
    "celestial bodies, advancing our understanding of the possibilities for long-term human habitation \n",
    "beyond Earth.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80490a-d05a-44b9-85c2-678dbfb48aef",
   "metadata": {},
   "source": [
    "Then we specify the question that we want answered. The model will identify the answer span within the source document that best answers this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f277c5-834e-4d01-a4ad-846dc0d2f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"What were the primary objectives of NASA’s Apollo missions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a3ecd-2879-4005-bdd3-1676425ffe95",
   "metadata": {},
   "source": [
    "Finally, we pass both the source text and the question to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250547f2-67e2-489e-8cf9-cebaf43f9b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.789436399936676, 'start': 172, 'end': 234, 'answer': 'landing humans \\non the Moon and safely returning them to Earth'}\n"
     ]
    }
   ],
   "source": [
    "answer = answerer(\n",
    "    question = question_text,\n",
    "    context = context_text\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550531a-c2ca-4dd0-b55e-aaa75fed059e",
   "metadata": {},
   "source": [
    "The pipeline processes the provided context and question, then returns a dictionary with keys such as `'answer'`, `'score'`, `'start'`, and `'end'`. These indicate the extracted answer, the model’s confidence score, and the character positions of the answer span in the context.\n",
    "\n",
    "Let's reformat the output to make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f79d864-a0e7-4fc3-92c7-e433f6c72e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What were the primary objectives of NASA’s Apollo missions?\n",
      "Answer: 'landing humans on the Moon and safely returning them to Earth'\n",
      "Score: 0.789436399936676, Start: 172, End: 234\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {question_text}\")\n",
    "answer_text = answer['answer'].replace('\\n','')\n",
    "print(f\"Answer: '{answer_text}'\")\n",
    "print(f\"Score: {answer['score']}, Start: {answer['start']}, End: {answer['end']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb464b4d-dbf6-4038-8d3d-b9c391360b93",
   "metadata": {},
   "source": [
    "This reformatting improves readability and enhances the interpretability of the model’s output by clearly presenting both the extracted answer and relevant details about its context.\n",
    "\n",
    "It is important to understand that the extractive QA approach used here retrieves answers directly from a specific span of the provided text. This ensures that responses remain verifiable and fully supported by the original source. However, this method is relatively basic in its approach to question answering. In contrast, an abstractive model generates responses in its own words, often producing more concise and natural-sounding answers. While this can improve readability, it also comes with drawbacks. An abstractive model may introduce information that is not explicitly stated in the original text, potentially leading to inaccuracies or hallucinated details.\n",
    "\n",
    "The choice between extractive and abstractive question answering ultimately depends on the application's needs. Extractive QA is ideal for scenarios requiring precise, evidence-backed answers, ensuring factual accuracy. On the other hand, abstractive QA offers greater flexibility and fluency, though it may sometimes prioritize readability over strict factual precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
